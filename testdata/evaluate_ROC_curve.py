#!/usr/bin/python3

"""
  This script loads a dump of a search database and a symbol file. From this,
  it generates the TPR, FPR, IRR's for both exact search (without using the
  permutation-based LSH) and approximate search.

  TPR - # of retrieved relevant / # total relevant
  FPR - # of retrieved irrelevant / # total irrelevant
  IRR - # of retrieved irrelevant / # total retrieved

  In the default setting (which is used for the untrained approach), the script
  does the following:

  1. Group all functions that share the same symbol together.
  2. Select 300 random function groups.
  3. Pick a random element from each function group.
  4. Perform a search (both precise and approximate), treat the other members
     of the same function group as the "relevant" examples, and everything else
     as irrelevant.

  If we wish to evaluate the performance on trained data, our situation is a bit
  different - we need to provide a list of functions that were not know at
  training time instead of step (2) and (3) above. Instead, we need to do the
  following:

  1. Group all functions that share the same symbol together.
  2. Load a list of functions that were unknown at training time, and obtain
     their relevant function groups.
  3. Pick the unknown function.
  4. Perform a search and proceed as above.

"""
from collections import defaultdict
import random, sys
import functionsimsearchutil
import functionsimsearch
import numpy as np
from absl import app
from absl import flags

flags.DEFINE_string('symbols', 'symbols.txt', 'Symbols file in the same ' +\
  'format as generated by the ./generate_training_data.py. Needs to contain ' +\
  'symbols for all the functions.')

flags.DEFINE_string('dbdump', 'dbdump.txt', 'Database dump in the same ' +\
  'format as generated by dumpfunctionindex -- a 5-column text file with the ' +\
  'first column zero, the 2nd and 3rd the SimHash, then file-id and address')

flags.DEFINE_string('index', 'db.index', 'The search index that the previous ' +\
  'two arguments depend on / are generated from.')

flags.DEFINE_bool('verbose', False, 'Dump debugging information.')

flags.DEFINE_bool('trained', False, 'Use mode for dealing w trained data.')

flags.DEFINE_string('validation_directory', 
  '/media/thomasdullien/september_20_train/validation_data_seen/',
  'A list of the functions that were not visible at training time.')

FLAGS = flags.FLAGS

def log(s):
  if FLAGS.verbose:
    print(s)

class SearchResult:
  def __init__(self, distance, function_info):
    self.distance = distance
    self.simhash = function_info[0]
    self.function_name = function_info[1]
    self.file_name = function_info[2]
    self.file_id = function_info[3]
    self.address = function_info[4]
  def __str__(self):
    retval = "{ distance: %d, simhash: %s, function_name: %s, file_name: %s, " +\
      "file_id: %s, address: %s"
    retval = retval % (self.distance, self.simhash, self.function_name,
      self.file_name, self.file_id, self.address)
    return retval

class LabeledDataManager:
  def __init__(self, symbols, dbdump, index):
    self.total_number_of_functions = 0
    # A dictionary that maps a tuple (file_id, address) to the full description
    # of that function (e.g. a tuple consisting of (hash, function_sym, file_name,
    # file_id, address).
    self.function_lookup = {}
    # Populate this dictionary. Retrieve functions and file ids and addresses.
    functions = functionsimsearchutil.read_inputs(symbols, dbdump, True)
    self.function_lookup = {}
    for function in functions:
      file_id = int(function[3], 16)
      address = int(function[4], 16)
      self.function_lookup[file_id, address] = function

    # A dictionary that maps a function name to a list of it's implementations.
    self.functions_to_implementations = defaultdict(list)
    for function in functions:
      function_sym = function[1]
      self.functions_to_implementations[function_sym].append(function)
      self.total_number_of_functions = self.total_number_of_functions + 1

    # The actual search index.
    self.index = functionsimsearch.SimHashSearchIndex(index, False, 50)

  def search_exact(self, simhash):
    """
      Use precise search and return a sorted list of all the functions and the
      respective distances.
    """
    hash_A, hash_B = self.split_uint128(simhash)
    resultlist = []
    for function_list in self.functions_to_implementations.values():
      for function in function_list:
        other_hash_A, other_hash_B = self.split_uint128(function[0])
        distance = 128.0-functionsimsearch.distance(hash_A, hash_B, other_hash_A,
          other_hash_B)
        resultlist.append(SearchResult(distance, function))
    resultlist.sort(key = lambda x : (x.distance, x.simhash), reverse = True)
    return resultlist

  def search_approximate(self, simhash):
    """
      Use the approximate search to return a list of functions and their
      distances.
    """
    hash_A, hash_B = self.split_uint128(simhash)
    resultlist = []
    results = self.index.query_top_N(hash_A, hash_B, 200000)
    total_results = len(results)
    missing_info = 0
    for element in results:
      result2 = self.function_lookup.get((element[1], element[2]))
      distance = element[0]
      if result2:
        resultlist.append(SearchResult(distance, result2))
      else:
        missing_info = missing_info + 1
    resultlist.sort(key = lambda x : (x.distance, x.simhash), reverse = True)
    return resultlist

  def split_uint128(self, simhash):
    result = (
      (simhash >> 64) & 0xFFFFFFFFFFFFFFFF,
      simhash & 0xFFFFFFFFFFFFFFFF)
    return result

  def function_info_to_file_id_address(self, function_info):
    """
      Convert a tuple with information about a function to a file_id, address
      tuple.
    """
    return (int(function_info[3], 16), int(function_info[4], 16))

  def count_relevant_results(self, function_name, result_list):
    implementations = self.functions_to_implementations[function_name]
    temporary_dict = {}
    relevant_count = 0
    for implementation in implementations:
      temporary_dict[self.function_info_to_file_id_address(implementation)] = \
        implementation
    for result in result_list:
      if temporary_dict.get((int(result.file_id, 16), int(result.address, 16))):
        relevant_count = relevant_count + 1
    return relevant_count

  def get_function_subset(self, number, get_all = False):
    if not FLAGS.trained:
      log("Untrained mode, checking for %d functions (%d max)." % (number,
        len(self.functions_to_implementations.keys())))
      if get_all or number > len(self.functions_to_implementations.keys()):
        return [x for x in self.functions_to_implementations.keys()]
      function_subset = np.random.choice(
        [ x for x in self.functions_to_implementations.keys() ], number,
        replace=False)
      return function_subset
    else:
      # Load the attract.txt from the validation data directory, extract the
      # second column and de-duplicate it.
      attract_txt = set([ line.split(" ")[1] for line in
        open(FLAGS.validation_directory + "/attract.txt", "rt").readlines() ])
      file_ids_and_addresses = [ (int(token.split(":")[0], 16),
          int(token.split(":")[1], 16)) for token in attract_txt]
      function_subset = []
      self.validation_symbol_to_function = {}
      for file_id, address in file_ids_and_addresses:
        function_sym = self.function_lookup[file_id, address][1]
        self.validation_symbol_to_function[function_sym] = (file_id, address)
        log("Validation function for %s is %lx %lx" % (function_sym, file_id,
          address))
        function_subset.append( function_sym )
      log("function_subset is %s" % function_subset.__str__())
      return function_subset

  def pick_implementation(self, function_name):
    if not FLAGS.trained:
      log("There are %d implementations of %s" % (
        len(self.functions_to_implementations[function_name]), function_name))
      return random.choice(self.functions_to_implementations[function_name])
    else:
      function_to_return = self.function_lookup[
        self.validation_symbol_to_function[function_name]]
      log("Returning function %s %lx %lx as implementation." % (function_name,
        self.validation_symbol_to_function[function_name][0],
        self.validation_symbol_to_function[function_name][1]))
      return function_to_return

  def get_implementations(self, function_name):
    return self.functions_to_implementations[function_name]

  def how_many_relevant_and_irrelevant(self, function_name):
    log("Determining how many relevant implementations exist for %s" % 
      function_name.__str__())
    for implementation in self.functions_to_implementations[function_name]:
      log(implementation.__str__())
    log("We have %d implementations." %
      len(self.functions_to_implementations[function_name]))
    relevant = len(self.functions_to_implementations[function_name])
    return (relevant, self.total_number_of_functions - relevant)

def main(argv):
  del argv # unused.

  # Refuse to run on Python less than 3.5 (unpredictable!).
  if sys.version_info[0] < 3 or sys.version_info[1] < 5:
    print("This script requires Python version 3.5 or higher.")
    sys.exit(1)

  data = LabeledDataManager(FLAGS.symbols, FLAGS.dbdump, FLAGS.index)

  log("Attempt to retrieve 300 random function families.")
  function_subset = data.get_function_subset(300, get_all=True)
  log("Got %d function families." % len(function_subset))

  exact_results = {}
  approximate_results = {}

  for distance in range(0, 128, 1):
    true_positive_rates_exact = []
    true_positive_rates_approx = []
    false_positive_rates_exact = []
    false_positive_rates_approx = []
    irrelevant_in_exact = []
    irrelevant_in_approx = []

    for function in function_subset:
      # Pick a random implementation of a given function.
      log("Picking function %s" % function.__str__())
      implementation = data.pick_implementation(function)
      simhash = implementation[0]

      log("Picked function with SimHash %lx %lx" % (data.split_uint128(simhash)))
      if not exact_results.get(function):
        exact_results[function] = data.search_exact(simhash)
      if not approximate_results.get(function):
        approximate_results[function] = data.search_approximate(simhash)

      log("exact_results[%s] are %s" % (function,
        [(x.distance, x.simhash) for x in exact_results[function][0:10]]))
      exact = [result for result in exact_results[function] if result.distance >=
        (128.0 - distance)]

      log("approximate_results[%s] are %s" % (function,
        approximate_results[function][0:10].__str__()))
      approximate = [result for result in approximate_results[function] if
        result.distance >= (128.0 - distance)]

      log("Exact search at distance %d yielded %d results" % (distance,
        len(exact)))
      log("Approximate search at distance %d yielded %d results" % (
        distance, len(approximate)))

      relevant_exact = data.count_relevant_results(function, exact)
      relevant_approximate = data.count_relevant_results(function, approximate)
      total_relevant, total_irrelevant = \
        data.how_many_relevant_and_irrelevant(function)

      true_positive_rate_exact = relevant_exact / total_relevant
      count_of_irrelevant_exact_results = len(exact) - relevant_exact
      false_positive_rate_exact = count_of_irrelevant_exact_results / \
        total_irrelevant

      true_positive_rate_approx = relevant_approximate / total_relevant
      count_of_irrelevant_approx_results = len(approximate) - relevant_approximate
      false_positive_rate_approx = count_of_irrelevant_approx_results / \
        total_irrelevant

      percentage_of_irrelevant_exact = count_of_irrelevant_exact_results / len(exact)
      percentage_of_irrelevant_approx = count_of_irrelevant_approx_results / len(approximate)

      true_positive_rates_exact.append(true_positive_rate_exact)
      true_positive_rates_approx.append(true_positive_rate_approx)
      false_positive_rates_exact.append(false_positive_rate_exact)
      false_positive_rates_approx.append(false_positive_rate_approx)
      irrelevant_in_exact.append(percentage_of_irrelevant_exact)
      irrelevant_in_approx.append(percentage_of_irrelevant_approx)

    tpr_exact = np.mean(true_positive_rates_exact)
    fpr_exact = np.mean(false_positive_rates_exact)
    tpr_approx = np.mean(true_positive_rates_approx)
    fpr_approx = np.mean(false_positive_rates_approx)
    irrelevant_exact = np.mean(irrelevant_in_exact)
    irrelevant_approx = np.mean(irrelevant_in_approx)
    print("%d %f %f %f %f %f %f" % (distance, tpr_exact, fpr_exact, tpr_approx,
      fpr_approx, irrelevant_exact, irrelevant_approx))

if __name__ == '__main__':
  app.run(main)

